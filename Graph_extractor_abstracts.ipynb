{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb5rOE7coY4C",
        "outputId": "3630141a-ae84-43a4-81db-4dd52a33d5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_dir = '/content/drive/My Drive/abstracts'\n",
        " #directory of all folders with the abstracts\n",
        " #directory is structured like:\n",
        " #/abstracts/\n",
        " #          /1992\n",
        " #          /1993\n",
        " #          /1994\n",
        " #          /1995\n",
        " #          /1996\n",
        " #          /1997\n",
        " #          /1998\n",
        " #          /1999\n",
        " #          /2000\n",
        " #          /2001\n",
        " #          /2002\n",
        " #          /2003"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mkIPkgdVZ3b"
      },
      "source": [
        "Use GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCkeS9M331fv",
        "outputId": "de9bc198-a224-4dc7-d41f-300eae5b123c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not availble\n",
            "SELECTED DEVICE: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import nltk\n",
        "#nltk is a natural language tool helpful for tokenizing\n",
        "\n",
        "nltk.download(\"book\")\n",
        "### Check if a cuda GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU availble')\n",
        "    # Define the device (here you can select which GPU to use if more than 1)\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print('GPU not availble')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"SELECTED DEVICE: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ouvw3amgCj8p"
      },
      "source": [
        "Install StellarGraph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blDdF3wTskZt"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  %pip install -q stellargraph[demos]==1.2.1\n",
        "\n",
        "import stellargraph as sg\n",
        "\n",
        "try:\n",
        "    sg.utils.validate_notebook_version(\"1.2.1\")\n",
        "except AttributeError:\n",
        "    raise ValueError(\n",
        "        f\"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n",
        "    ) from None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLuR7gGu_VOx"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import stellargraph as sg\n",
        "from stellargraph.data import EdgeSplitter\n",
        "from stellargraph.mapper import GraphSAGELinkGenerator\n",
        "from stellargraph.layer import GraphSAGE, link_classification\n",
        "from stellargraph.calibration import expected_calibration_error, plot_reliability_diagram\n",
        "from stellargraph.calibration import IsotonicCalibration, TemperatureCalibration\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn import preprocessing, feature_extraction, model_selection\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "from stellargraph import globalvar\n",
        "from stellargraph import datasets\n",
        "from IPython.display import display, HTML\n",
        "from  stellargraph import StellarGraph\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyAw7tEiimiJ"
      },
      "source": [
        "Graph extraction block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ihHNHVXr6lc",
        "outputId": "5cc0daf7-5524-441a-a919-e77c7c4dcff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing abstracts of year 1992\n",
            "Processing abstracts of year 1993\n",
            "Processing abstracts of year 1994\n",
            "Processing abstracts of year 1995\n",
            "Processing abstracts of year 1996\n",
            "Processing abstracts of year 1997\n",
            "Processing abstracts of year 1998\n",
            "Processing abstracts of year 1999\n",
            "Processing abstracts of year 2000\n",
            "Processing abstracts of year 2001\n",
            "Processing abstracts of year 2002\n",
            "Processing abstracts of year 2003\n",
            "Number of connected components: 931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-d347fb2fb220>:136: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
            "  node_feats.append(nx.pagerank_numpy(Gs))\n",
            "/usr/local/lib/python3.8/dist-packages/networkx/algorithms/link_analysis/pagerank_alg.py:354: FutureWarning: google_matrix will return an np.ndarray instead of a np.matrix in\n",
            "NetworkX version 3.0.\n",
            "  M = google_matrix(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StellarGraph: Undirected multigraph\n",
            " Nodes: 12061, Edges: 23834\n",
            "\n",
            " Node types:\n",
            "  default: [12061]\n",
            "    Features: float32 vector, length 10\n",
            "    Edge types: default-default->default\n",
            "\n",
            " Edge types:\n",
            "    default-default->default: [23834]\n",
            "        Weights: all 1 (default)\n",
            "        Features: float32 vector, length 3\n"
          ]
        }
      ],
      "source": [
        "nodes = []        #list of all nodes\n",
        "nodes_feat = {}   #dictionary of all node features to use for networkx\n",
        "edge_feats = {}   #dictionary of all edge features to use for networkx\n",
        "source = []       #first end of the edges\n",
        "target = []       #second end of the edges\n",
        "\n",
        "\n",
        "os.chdir(dataset_dir)\n",
        "for file in os.listdir(): #for cycle that iterates the folders of the years\n",
        "  dir_name = str(file)    #year \n",
        "  os.chdir(dataset_dir + \"/\" + dir_name)\n",
        "  print(\"Processing abstracts of year \" + dir_name)\n",
        "  for file in os.listdir(): #for cycle that iterates the abstracts of the papers\n",
        "    abstract = dataset_dir + \"/\" + dir_name + \"/\" + str(file)\n",
        "    with open(abstract, \"r\") as file1:    \n",
        "      authors = []                        #list of all authors of one paper\n",
        "      for line in file1:                  #for cycle that iterates the lines of the abstract file                   \n",
        "        tokens = nltk.word_tokenize(line) #tokenize line\n",
        "        if(len(tokens) > 0):\n",
        "          if(tokens[0] == \"Authors\" or tokens[0] == \"Author\"): #check if we are on the authors line\n",
        "            author = \"\"\n",
        "            skippable = False              #true if we are inside two parenthesis, so not an author name\n",
        "            for i in range(2,len(tokens)): #skip \"Authors\" and \":\"\n",
        "              if(tokens[i][0] == \"(\"):     #we are now inside two parenthesis\n",
        "                skippable = True\n",
        "              if(not(skippable)):\n",
        "                if(tokens[i] != \"and\" and tokens[i] != \",\"): #ignore \"and\" and \",\"\n",
        "                  author = author + tokens[i]\n",
        "                else:\n",
        "                  if(len(author) != 0):\n",
        "                    authors.append(author)\n",
        "                  author = \"\"\n",
        "              if(tokens[i][len(tokens[i]) - 1] == \")\"):      #end of parenthesis\n",
        "                skippable = False\n",
        "            if(len(author) != 0):      #add last author, if any\n",
        "              authors.append(author)\n",
        "            break                      #authors are only in one row, we can skip the rest\n",
        "      for i in range(0, len(authors)): #add new nodes \n",
        "        if(not(authors[i] in nodes)):\n",
        "          nodes.append(authors[i])\n",
        "          nodes_feat.update({authors[i] : {\"author_code\" : hash(authors[i])}})\n",
        "\n",
        "      for i in range(0, len(authors)):  #check every combination of two nodes\n",
        "        for j in range(i+1, len(authors)):\n",
        "          if(not((authors[i], authors[j]) in edge_feats) and not((authors[j], authors[i]) in edge_feats)): #add new edges\n",
        "            source.append(authors[i])\n",
        "            target.append(authors[j])\n",
        "            edge_feats.update({(authors[i], authors[j]):{\"year\" : int(dir_name), \"collabs\" : 1}})\n",
        "          else: \n",
        "            if ((authors[i], authors[j]) in edge_feats.keys()):\n",
        "              if(edge_feats.get((authors[i], authors[j])).get(\"year\") ==int(dir_name)):  #check if there were already collaborations between i and j this year\n",
        "                n_coll = edge_feats.get((authors[i], authors[j])).get(\"collabs\")\n",
        "                edge_feats.update({(authors[i], authors[j]):{\"year\" : int(dir_name), \"collabs\" : n_coll + 1}})\n",
        "            if ((authors[j], authors[i]) in edge_feats.keys()):\n",
        "              if(edge_feats.get((authors[j], authors[i])).get(\"year\") ==int(dir_name)):\n",
        "                n_coll = edge_feats.get((authors[j], authors[i])).get(\"collabs\")\n",
        "                edge_feats.update({(authors[j], authors[i]):{\"year\" : int(dir_name), \"collabs\" : n_coll + 1}})\n",
        "\n",
        "\n",
        "author_codes = [] #list of hash codes of the authors names\n",
        "years = []        #list of years corresponding to the edges\n",
        "collabs = []      #list of collaborations corresponding to the edges\n",
        "nodes_index = []  #list of names of the nodes\n",
        "edges = []        #list of all edges\n",
        "\n",
        "for element in edge_feats:  #split edge_feats into lists to use for StellarGraph\n",
        "  edges.append((element[0], element[1], dict(year=edge_feats.get(element).get(\"year\"), collabs=edge_feats.get(element).get(\"collabs\"))))\n",
        "  years.append(edge_feats.get(element).get(\"year\"))\n",
        "  collabs.append(edge_feats.get(element).get(\"collabs\"))\n",
        "\n",
        "G = nx.MultiGraph()  #original graph\n",
        "Gs = nx.MultiGraph() #graph without nodes with degree = 0\n",
        "\n",
        "G.add_nodes_from(nodes)  #add nodes\n",
        "nx.set_node_attributes(G, nodes_feat)  #add node features\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "Gs.add_nodes_from(nodes)\n",
        "nx.set_node_attributes(Gs, nodes_feat)\n",
        "Gs.add_edges_from(edges)\n",
        "\n",
        "\n",
        "for n in G.nodes:  #remove nodes with degree = 0\n",
        "  if(nx.degree(G,n) == 0):\n",
        "    Gs.remove_node(n)     \n",
        "  else:\n",
        "    nodes_index.append(n)\n",
        "\n",
        "\n",
        "print(\"Number of connected components: \" + str(nx.number_connected_components(Gs)))\n",
        "node_feats = [] #list of all lists of node features to use for StellarGraph\n",
        "\n",
        "#list of graph features to use for StellarGraph\n",
        "deg = []\n",
        "har_cen = []\n",
        "deg_cen = []\n",
        "clos_cen = []\n",
        "bet_cen = []\n",
        "load_cen = []\n",
        "square_clus = []\n",
        "pagerank = []\n",
        "constr = []\n",
        "\n",
        "edge_load_cen = []\n",
        "\n",
        "#graph features are calculated just once, to speed up the program\n",
        "node_feats.append(nx.get_node_attributes(Gs,\"author_code\"))\n",
        "node_feats.append(nx.degree(Gs))\n",
        "node_feats.append(nx.harmonic_centrality(Gs))\n",
        "node_feats.append(nx.degree_centrality(Gs))\n",
        "node_feats.append(nx.closeness_centrality(Gs))\n",
        "node_feats.append(nx.betweenness_centrality(Gs))\n",
        "node_feats.append(nx.load_centrality(Gs))\n",
        "node_feats.append(nx.square_clustering(Gs))\n",
        "node_feats.append(nx.pagerank_numpy(Gs))\n",
        "node_feats.append(nx.constraint(Gs))\n",
        "\n",
        "dict_edge_load_cen = nx.edge_load_centrality(Gs)\n",
        "\n",
        "\n",
        "for n in Gs.nodes:  #add graph features in their respective list\n",
        "  author_codes.append(node_feats[0][n])\n",
        "  deg.append(node_feats[1][n])\n",
        "  har_cen.append(node_feats[2][n])\n",
        "  deg_cen.append(node_feats[3][n])\n",
        "  clos_cen.append(node_feats[4][n])\n",
        "  bet_cen.append(node_feats[5][n])\n",
        "  load_cen.append(node_feats[6][n])\n",
        "  square_clus.append(node_feats[7][n])\n",
        "  pagerank.append(node_feats[8][n])\n",
        "  constr.append(node_feats[9][n])\n",
        "\n",
        "for e in Gs.edges:\n",
        "  edge_load_cen.append(dict_edge_load_cen[e[0:2]])\n",
        "\n",
        "square_node_data = pd.DataFrame( #data structure for the nodes\n",
        "    {\"author_code\": author_codes,\"degree\":deg, \"harmonic_centrality\":har_cen, \"degree_centrality\":deg_cen, \n",
        "    \"closeness_centrality\": clos_cen, \"betweenness_centrality\":bet_cen, \"load_centrality\":load_cen,\n",
        "     \"square_clustering\":square_clus,  \"pagerank\": pagerank,  \"constraint\": constr},\n",
        "     index = nodes_index)\n",
        "\n",
        "square_edges = pd.DataFrame( #data structure for the edges\n",
        "    {\"source\": source, \"target\": target, \"year\": years, \"collab\": collabs, \"load_centrality\": edge_load_cen}\n",
        ")\n",
        "\n",
        "G_stel = StellarGraph(square_node_data, square_edges) #build graph\n",
        "print(G_stel.info())\n",
        "\n",
        "#saving the data\n",
        "np.save(\"/content/author_codes.npy\",author_codes)\n",
        "np.save(\"/content/deg.npy\",deg)\n",
        "np.save(\"/content/har_cen.npy\",har_cen)\n",
        "np.save(\"/content/deg_cen.npy\",deg_cen)\n",
        "np.save(\"/content/clos_cen.npy\",clos_cen)\n",
        "np.save(\"/content/bet_cen.npy\",bet_cen)\n",
        "np.save(\"/content/load_cen.npy\",load_cen)\n",
        "np.save(\"/content/square_clus.npy\",square_clus)\n",
        "np.save(\"/content/pagerank.npy\",pagerank)\n",
        "np.save(\"/content/constr.npy\",constr)\n",
        "np.save(\"/content/nodes_index.npy\",nodes_index)\n",
        "np.save(\"/content/source.npy\",source)\n",
        "np.save(\"/content/target.npy\",target)\n",
        "np.save(\"/content/years.npy\",years)\n",
        "np.save(\"/content/collabs.npy\",collabs)\n",
        "np.save(\"/content/edge_load_cen.npy\",edge_load_cen)\n",
        "np.save(\"/content/G_stel.npy\",G_stel)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}